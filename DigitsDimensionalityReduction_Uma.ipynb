{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will use another digit data set, since it only consists of numerical features. Your job is to apply the dimension reduction technique we learned, and combined with the classification methods you learned from DS861, to build a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "from sklearn.datasets import load_digits\n",
    "X_digits, y_digits = load_digits(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state = 862)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will build your classifier with the appropriate dimension reduction technique and classification model. Recall that we have learned the following classifiers in DS861: Logistic Regression, Decision Tree, Random Forest, Boosting, KNN. This data set is a multi-level data set, hence you should think about which model is appropriate (or not appropriate). To save time, you may just pick two estimators that you think is the most appropriate and compare.\n",
    "\n",
    "Some general rules you should follow:\n",
    "\n",
    "1. Tune your dimension reduction technique\n",
    "2. Tune your model\n",
    "3. Select your hyperparameters based on a hold-out set (either via CV or train/validate/test split)\n",
    "4. Report the accuracy on the test set\n",
    "\n",
    "You may ignore randomized PCA and incremental PCA, since they don't have added value here. We will not use MDS and t-SNE here as well since they cannot be used to transform new observations. If you want to write a wrapper that contains multiple classifiers, [here](https://stackoverflow.com/questions/50285973/pipeline-multiple-classifiers) or [here](https://stackoverflow.com/questions/38555650/try-multiple-estimator-in-one-grid-search) have some good examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the classification models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements:\n",
    "\n",
    "#GridSearchCV and Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Accuracy score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of classification models:\n",
    "classification_models = []\n",
    "classification_models.append(KNeighborsClassifier())\n",
    "classification_models.append(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of names of those classification models:\n",
    "classification_model_names = [\n",
    "         \"K Nearest Neighbours\",\n",
    "         \"Random Forest\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning parameters for GridSearchCV\n",
    "parameters = [\n",
    "    {\n",
    "        #KNN\n",
    "        \"kpca__n_components\": range(2, 20),\n",
    "        \"kpca__kernel\": [\"linear\",\"rbf\", \"sigmoid\",\"poly\"],\n",
    "        \"clf__n_neighbors\": range(5,16)\n",
    "    },\n",
    "    {\n",
    "        #RandomForest\n",
    "        \"kpca__n_components\": range(2, 20),\n",
    "        \"kpca__kernel\": [\"linear\",\"rbf\", \"sigmoid\",\"poly\"],\n",
    "        \"clf__n_estimators\": [50,100,150]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION ALGORITHMS PERFORMANCE WITH KERNEL PCA DIMENSIONALITY REDUCTION: \n",
      "------------------------------------------------------------------------------------\n",
      "MODEL NAME             |ACCURACY SCORE    |\tPARAMETERS          \n",
      "------------------------------------------------------------------------------------\n",
      "K Nearest Neighbours   |0.98              |{'clf__n_neighbors': 5, 'kpca__kernel': 'linear', 'kpca__n_components': 19}\n",
      "Random Forest          |0.9666666666666667|{'clf__n_estimators': 50, 'kpca__kernel': 'linear', 'kpca__n_components': 19}\n"
     ]
    }
   ],
   "source": [
    "#Finding accuracies for classification algorithms with Kernel PCA Dimensionality Reduction\n",
    "print(\"CLASSIFICATION ALGORITHMS PERFORMANCE WITH KERNEL PCA DIMENSIONALITY REDUCTION: \")\n",
    "print(\"------------------------------------------------------------------------------------\")  \n",
    "print (\"{:<22} |{:<18}|\\t{:<20}\".format('MODEL NAME','ACCURACY SCORE','PARAMETERS'))\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "for name, classifier, params in zip(classification_model_names, classification_models, parameters):\n",
    "    \n",
    "    clf_pipe = Pipeline([\n",
    "        (\"kpca\", KernelPCA(random_state = 862)),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    gs_clf = GridSearchCV(clf_pipe, param_grid=params, n_jobs=-1)\n",
    "    clf = gs_clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    best_params = gs_clf.best_params_\n",
    "    \n",
    "    #Getting the best accuracy score of the classification model\n",
    "    print (\"{:<22} |{:<18}|{}\".format(name, score, best_params))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements:\n",
    "\n",
    "#LLE\n",
    "from sklearn.manifold import LocallyLinearEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning parameters for GridSearchCV\n",
    "parameters = [\n",
    "    {\n",
    "        #KNN\n",
    "        \"lle__n_components\": range(2,20),\n",
    "        \"lle__n_neighbors\": [5,10],\n",
    "        \"clf__n_neighbors\": range(5,16)\n",
    "    },\n",
    "    {\n",
    "        #Random Forest\n",
    "        \"lle__n_components\": range(2,20),\n",
    "        \"lle__n_neighbors\": [5,10],\n",
    "        \"clf__n_estimators\": [50,100,150]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION ALGORITHMS PERFORMANCE WITH LLE DIMENSIONALITY REDUCTION: \n",
      "------------------------------------------------------------------------------------\n",
      "MODEL NAME                |ACCURACY SCORE      |\tPARAMETERS          \n",
      "------------------------------------------------------------------------------------\n",
      "K Nearest Neighbours      |0.9666666666666667  |{'clf__n_neighbors': 15, 'lle__n_components': 15, 'lle__n_neighbors': 5}\n",
      "Random Forest             |0.9777777777777777  |{'clf__n_estimators': 150, 'lle__n_components': 19, 'lle__n_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "#Finding accuracies for classification algorithms with LLE Dimensionality Reduction\n",
    "print(\"CLASSIFICATION ALGORITHMS PERFORMANCE WITH LLE DIMENSIONALITY REDUCTION: \")\n",
    "print(\"------------------------------------------------------------------------------------\")  \n",
    "print (\"{:<25} |{:<20}|\\t{:<20}\".format('MODEL NAME','ACCURACY SCORE','PARAMETERS'))\n",
    "print(\"------------------------------------------------------------------------------------\")  \n",
    "for name, classifier, params in zip(classification_model_names, classification_models, parameters):\n",
    "    \n",
    "    clf_pipe = Pipeline([\n",
    "         (\"lle\", LocallyLinearEmbedding(random_state = 862)),\n",
    "         ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    gs_clf = GridSearchCV(clf_pipe, param_grid=params, n_jobs=-1)\n",
    "    clf = gs_clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    best_params = gs_clf.best_params_\n",
    "    \n",
    "    #Getting the best accuracy score of the classification model\n",
    "    print(\"{:<25} |{:<20}|{}\".format(name, score, best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statements:\n",
    "\n",
    "#Isomap\n",
    "from sklearn.manifold import Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning parameters for GridSearchCV\n",
    "parameters = [       \n",
    "    {\n",
    "        #KNN\n",
    "        \"iso__n_components\": range(2,20),\n",
    "        \"iso__n_neighbors\":[5,10],\n",
    "        \"clf__n_neighbors\": range(5,16)\n",
    "    },\n",
    "    {\n",
    "        #Random Forest\n",
    "        \"iso__n_components\": range(2,20),\n",
    "        \"iso__n_neighbors\":[5,10],\n",
    "        \"clf__n_estimators\": [50,100,150]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION ALGORITHMS PERFORMANCE WITH ISOMAP DIMENSIONALITY REDUCTION: \n",
      "------------------------------------------------------------------------------------\n",
      "MODEL NAME             |ACCURACY SCORE     |\tPARAMETERS          \n",
      "------------------------------------------------------------------------------------\n",
      "K Nearest Neighbours   |0.9755555555555555 |{'clf__n_neighbors': 5, 'iso__n_components': 15, 'iso__n_neighbors': 5}\n",
      "Random Forest          |0.9688888888888889 |{'clf__n_estimators': 100, 'iso__n_components': 18, 'iso__n_neighbors': 5}\n"
     ]
    }
   ],
   "source": [
    "#Finding accuracies for classification algorithms with Isomap Dimensionality Reduction\n",
    "print(\"CLASSIFICATION ALGORITHMS PERFORMANCE WITH ISOMAP DIMENSIONALITY REDUCTION: \")\n",
    "print(\"------------------------------------------------------------------------------------\")  \n",
    "print (\"{:<22} |{:<18} |\\t{:<20}\".format('MODEL NAME','ACCURACY SCORE','PARAMETERS'))\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "for name, classifier, params in zip(classification_model_names, classification_models, parameters):\n",
    "    \n",
    "    clf_pipe = Pipeline([\n",
    "        (\"iso\", Isomap()),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    gs_clf = GridSearchCV(clf_pipe, param_grid=params, n_jobs=-1)\n",
    "    clf = gs_clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    best_params = gs_clf.best_params_\n",
    "    #Getting the best accuracy score of the classification model\n",
    "    print(\"{:<22} |{:<18} |{}\".format(name, score, best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the best combination according to your accuracy score on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: Kernel PCA with KNN. Accuracy = 0.98\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the original data set and the two classifers you chose, run the procedure again, but this time without any dimension reduction. Make sure you tune your classifiers. Which result is better? Using the original data set or the reduced data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning parameters for GridSearchCV\n",
    "parameters = [\n",
    "    {\n",
    "        #KNN\n",
    "        \"clf__n_neighbors\": range(5,16)\n",
    "    },\n",
    "    {\n",
    "        #Random Forest\n",
    "       'clf__n_estimators': [50,100,150,200,250,300]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION ALGORITHMS PERFORMANCE WITHOUT DIMENSIONALITY REDUCTION: \n",
      "------------------------------------------------------------------------------------\n",
      "MODEL NAME                |\tACCURACY SCORE      \t|\tPARAMETERS\n",
      "------------------------------------------------------------------------------------\n",
      "K Nearest Neighbours      |\t0.98                \t|\t{'clf__n_neighbors': 5}\n",
      "Random Forest             |\t0.9711111111111111  \t|\t{'clf__n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "#Finding accuracy of various classification algorithms without Dimensionality Reduction \n",
    "print(\"CLASSIFICATION ALGORITHMS PERFORMANCE WITHOUT DIMENSIONALITY REDUCTION: \")\n",
    "print(\"------------------------------------------------------------------------------------\")  \n",
    "print (\"{:<25} |\\t{:<20}\\t|\\t{:<10}\".format('MODEL NAME','ACCURACY SCORE','PARAMETERS'))\n",
    "print(\"------------------------------------------------------------------------------------\")  \n",
    "for name, classifier, params in zip(classification_model_names, classification_models, parameters):\n",
    "    \n",
    "    clf_pipe = Pipeline([\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "    \n",
    "    gs_clf = GridSearchCV(clf_pipe, param_grid=params, n_jobs=-1)\n",
    "    clf = gs_clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    best_params = gs_clf.best_params_\n",
    "    #Getting the best accuracy score of the classification model\n",
    "    print(\"{:<25} |\\t{:<20}\\t|\\t{}\".format(name, score, best_params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which result is better? Using the original data set or the reduced data set? - \n",
    "#### Using the Original dataset we get an accuracy of 0.98. \n",
    "#### Using Dimensionality Reduction, we got an accuracy of 0.98 with KNN- Linear Kernel PCA, 0.977 with Random Forest-LLE. \n",
    "#### Even though there is not much difference in the accuracy. I will choose LLE Dimensionality Reduction with Random Forest Classification giving an accuracy of 0.977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
